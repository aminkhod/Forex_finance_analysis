{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16, 100), (100, 200), (200, 1)]\n",
      "[[0.49457948 0.50542052]\n",
      " [0.49457948 0.50542052]\n",
      " [0.49457948 0.50542052]\n",
      " ...\n",
      " [0.49457948 0.50542052]\n",
      " [0.49457948 0.50542052]\n",
      " [0.49457948 0.50542052]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       655\n",
      "         1.0       0.48      1.00      0.65       610\n",
      "\n",
      "   micro avg       0.48      0.48      0.48      1265\n",
      "   macro avg       0.24      0.50      0.33      1265\n",
      "weighted avg       0.23      0.48      0.31      1265\n",
      "\n",
      "[[  0 655]\n",
      " [  0 610]]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\metrics\\ranking.py:656: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\metrics\\ranking.py:526: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16, 15), (15, 1)]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      1.00      0.68       655\n",
      "         1.0       0.00      0.00      0.00       610\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      1265\n",
      "   macro avg       0.26      0.50      0.34      1265\n",
      "weighted avg       0.27      0.52      0.35      1265\n",
      "\n",
      "[[655   0]\n",
      " [610   0]]\n",
      "2\n",
      "16\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "###reading data\n",
    "missing_value=['?']\n",
    "data2= pd.read_csv(\"EC-H1-train.csv\", na_values=missing_value)\n",
    "data1= pd.read_csv(\"EC-H1-test.csv\", na_values=missing_value)\n",
    "##replacing\n",
    "# bmedian = data2['Bare Nuclei'].median()\n",
    "# data2['Bare Nuclei'].fillna(bmedian,inplace=True)\n",
    "\n",
    "X_train=data2.values[:,1:]\n",
    "y_train=data2.values[:,0]\n",
    "X_test=data1.values[:,1:]\n",
    "y_test=data1.values[:,0]\n",
    "# y=np.array([1 if yinstance==4 else 0 for yinstance in y ])\n",
    "\n",
    "\n",
    "\n",
    "###### Devide data to test and train\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "\n",
    "############## Regression with MLP#############\n",
    "#######sklearn.neural_network.MLPRegressor\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "#\n",
    "# MLPRegressor(hidden_layer_sizes=(100, ),activation='relu',solver='adam',\n",
    "# \t\t\t\talpha=0.0001, batch_size='auto',\n",
    "#               learning_rate='constant',learning_rate_init=0.001,\n",
    "#                 power_t=0.5, max_iter=200, shuffle=True,\n",
    "#               random_state=None, tol=0.0001, verbose=False,\n",
    "# \t\t\t\twarm_start=False,momentum=0.9,\n",
    "#               nesterovs_momentum=True, early_stopping=False,\n",
    "# \t\t\t\tvalidation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "#               epsilon=1e-08,\tn_iter_no_change=10)\n",
    "\n",
    "\n",
    "'''\n",
    "hidden_layer_sizes=(100,12,45 )\n",
    "activation=\n",
    "'identity', no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "'logistic', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "'tanh', the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "'relu', the rectified linear unit function, returns f(x) = max(0, x)\n",
    "\n",
    "solver=\n",
    "'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
    "'sgd' refers to stochastic gradient descent.\n",
    "'adam' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "alpha: L2 penalty (regularization term) parameter.\n",
    "batch_size='auto': When set to 'auto', batch_size=min(200, n_samples)\n",
    "learning_rate: 'constant', 'invscaling', 'adaptive'    Only used when solver='sgd'.\n",
    "max_iter : int, optional, default 200\n",
    "warm_start : bool, optional, default False\n",
    "momentum : float, default 0.9   only used when solver='sgd'.\n",
    "validation_fraction : float, optional, default 0.1\n",
    "beta_1 : float, optional, default 0.9   when solver='adam'\n",
    "beta_2 : float, optional, default 0.999   when solver='adam'\n",
    "'''\n",
    "\n",
    "############## Classification with MLP#############\n",
    "########sklearn.neural_network.MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "#               beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "#               epsilon=1e-08, hidden_layer_sizes=(5, 2),\n",
    "#               learning_rate='constant', learning_rate_init=0.001,\n",
    "#               max_iter=200, momentum=0.9, n_iter_no_change=10,\n",
    "#               nesterovs_momentum=True, power_t=0.5, random_state=1,\n",
    "#               shuffle=True, solver='lbfgs', tol=0.0001,\n",
    "#               validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(100, 200), random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict =clf.predict(X_test)\n",
    "\n",
    "print([coef.shape for coef in clf.coefs_])\n",
    "print(clf.predict_proba(X_test))\n",
    "print(classification_report(y_test,predict))\n",
    "print(confusion_matrix(y_test,predict))\n",
    "\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,predict, pos_label=2)\n",
    "mlpauc= metrics.auc(fpr, tpr)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.precision_recall_curve(y_test,predict, pos_label=2)\n",
    "mlpaupr= metrics.auc(fpr, tpr)\n",
    "print(mlpaupr)\n",
    "\n",
    "#Currently, MLPClassifier supports only the Cross-Entropy loss function\n",
    "#MLPClassifier supports multi-class classification by applying Softmax as the output function\n",
    "clf = MLPClassifier(solver='adam',hidden_layer_sizes=(15,), alpha=1e-5, random_state=0)\n",
    "mlp = clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = mlp.predict(X_test)\n",
    "print([coef.shape for coef in clf.coefs_])\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(len(mlp.coefs_))\n",
    "print(len(mlp.coefs_[0]))\n",
    "print(len(mlp.intercepts_[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[261 227]\n",
      " [394 383]]\n",
      "Accuracy: 0.509090909090909\n",
      "Precision: 0.4929214929214929\n",
      "Recall: 0.6278688524590164\n",
      "F_measure: 0.5522710886806056\n",
      "classification error is : 0.4909090909090909\n",
      "\u001b[43m\n",
      "\t\t\u001b[31mpredicted NO \t \u001b[31mpredicted YES \n",
      "\u001b[32mactual NO \t \u001b[34m328 \t\t  \u001b[30m327 \t\n",
      "\u001b[32mactual YES \t \u001b[30m300 \t\t  \u001b[34m310 \t\n",
      "\u001b[46m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.50      0.51       655\n",
      "         1.0       0.49      0.51      0.50       610\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      1265\n",
      "   macro avg       0.50      0.50      0.50      1265\n",
      "weighted avg       0.51      0.50      0.50      1265\n",
      "\n",
      "\u001b[49m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[287 368]\n",
      " [277 333]]\n",
      "\u001b[42m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.44      0.47       655\n",
      "         1.0       0.48      0.55      0.51       610\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      1265\n",
      "   macro avg       0.49      0.49      0.49      1265\n",
      "weighted avg       0.49      0.49      0.49      1265\n",
      "\n",
      "\u001b[49m\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree_classifier = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10)\n",
    "tree_model = tree_classifier.fit(X_train, y_train)\n",
    "predict = tree_model.predict(X_test)\n",
    "\n",
    "# import graphviz\n",
    "# dot_data = tree.export_graphviz(tree_model, out_file=None)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph.render(\"Autism\",view=True)\n",
    "names = list(data1)\n",
    "class_names = ['up', 'down']\n",
    "\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "#graphviz==> https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "\n",
    "dot_data = tree.export_graphviz(tree_classifier , out_file=None,\n",
    "                                feature_names=names[1:],\n",
    "                                class_names=class_names)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())\n",
    "\n",
    "# Create pdf\n",
    "graph.write_pdf(\"Finance.pdf\")\n",
    "\n",
    "# Create PNG\n",
    "graph.write_png(\"Finance.png\")\n",
    "\n",
    "\n",
    "####Confusion matrix\n",
    "cnf_matrix = metrics.confusion_matrix(predict,y_test )\n",
    "print(cnf_matrix)\n",
    "\n",
    "###Metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predict))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, predict))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, predict))\n",
    "print(\"F_measure:\",metrics.f1_score(y_test, predict))\n",
    "print( \"classification error is :\", np.sum(predict != y_test) / len(y_test) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## Bagging method  ######\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "bagging = BaggingClassifier()\n",
    "bagging_model = bagging.fit(X_train, y_train)\n",
    "\n",
    "bagging_predict = bagging_model.predict(X_test)\n",
    "confusion = metrics.confusion_matrix(y_test, bagging_predict)\n",
    "\n",
    "from colorama import Back, Fore\n",
    "print(Back.YELLOW)\n",
    "print(\"\\t\\t{0:s} \\t {1:s} \".format(Fore.RED + \"predicted NO\", Fore.RED + \"predicted YES\"))\n",
    "print(Fore.GREEN + \"actual NO \\t {0} \\t\\t  {1} \\t\".format(Fore.BLUE + str(confusion[0, 0]), Fore.BLACK + str(confusion[0, 1])))\n",
    "print(Fore.GREEN + \"actual YES \\t {0} \\t\\t  {1} \\t\".format(Fore.BLACK + str(confusion[1, 0]), Fore.BLUE + str(confusion[1, 1])))\n",
    "\n",
    "print(Back.CYAN)\n",
    "print(metrics.classification_report(y_test, bagging_predict))\n",
    "print(Back.RESET)\n",
    "\n",
    "\n",
    "\n",
    "########### Random Forest  ###########\n",
    "\n",
    "random_forest = RandomForestClassifier(min_samples_split=5, min_samples_leaf=2, max_depth=10)\n",
    "random_forest_model = random_forest.fit(X_train, y_train)\n",
    "\n",
    "random_forest_predict = random_forest_model.predict(X_test)\n",
    "print( metrics.confusion_matrix(y_test, random_forest_predict))\n",
    "\n",
    "print(Back.GREEN)\n",
    "print(metrics.classification_report(y_test, random_forest_predict))\n",
    "print(Back.RESET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
